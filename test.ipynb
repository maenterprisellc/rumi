{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc0a908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Read Corpus ...\n",
      "Location of clean data data/clean/Test/\n",
      "processed thread 1 / 1 ...\n",
      "[+] Chunking the text using Regex ...\n",
      "[+] do the estimated merges 4 ...\n",
      "\n",
      "                  Merge Done 1 / 4 \n",
      "                  Vocab size: 257   \n",
      "                  New Ids: 25    \n",
      "                  Work time for Get Pairs: 0.00 seconds  \n",
      "                  Work time for Merge: 0.00 seconds\n",
      "                  ______________________________________________________\n",
      "                  \n",
      "\n",
      "                  Merge Done 2 / 4 \n",
      "                  Vocab size: 258   \n",
      "                  New Ids: 25    \n",
      "                  Work time for Get Pairs: 0.00 seconds  \n",
      "                  Work time for Merge: 0.00 seconds\n",
      "                  ______________________________________________________\n",
      "                  \n",
      "\n",
      "                  Merge Done 3 / 4 \n",
      "                  Vocab size: 259   \n",
      "                  New Ids: 25    \n",
      "                  Work time for Get Pairs: 0.00 seconds  \n",
      "                  Work time for Merge: 0.00 seconds\n",
      "                  ______________________________________________________\n",
      "                  \n",
      "\n",
      "                  Merge Done 4 / 4 \n",
      "                  Vocab size: 260   \n",
      "                  New Ids: 25    \n",
      "                  Work time for Get Pairs: 0.00 seconds  \n",
      "                  Work time for Merge: 0.00 seconds\n",
      "                  ______________________________________________________\n",
      "                  \n"
     ]
    }
   ],
   "source": [
    "from tokenizer import mabpe\n",
    "special_tokens = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100276\n",
    "}\n",
    "tokenizer = mabpe.MARegexTokenizer(special_tokens=special_tokens, vocab_size=260)\n",
    "tokenizer.build_bpe('data/clean/Test/')\n",
    "#tokenizer.load('data/tokenizer/ma_bpe_wiki.json')\n",
    "#tokenizer.encode(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a146cbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4], [5, 6], [7], [8]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3],[4]]\n",
    "b = [[5,6],[7],[8]]\n",
    "a.extend(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1766022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 104, 278, 317, 257, 269, 116]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ece978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is test'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([84, 104, 278, 317, 257, 269, 116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29620f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/tokenizer/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b35ae54",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MARegexTokenizer' object has no attribute 'get_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vocab = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_vocab\u001b[49m(text)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MARegexTokenizer' object has no attribute 'get_vocab'"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e6151e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = mabpe.MARegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c6379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.load('data/tokenizer/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d93825d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 104, 278, 317, 257, 269, 116]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.encode(\"This is test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c064c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1]*22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e67f498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[array[i-5:i] for i in range(5,21+5,5) if array[i-5:i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 1})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "default_dict = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd98768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 3})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_dict['a'] += 1\n",
    "default_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd82876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location of clean data ../data/clean/wiki_crawl\n",
      "processed thread 1 / 200 ...\n",
      "processed thread 2 / 200 ...\n",
      "processed thread 3 / 200 ...\n",
      "processed thread 4 / 200 ...\n",
      "processed thread 5 / 200 ...\n",
      "processed thread 6 / 200 ...\n",
      "processed thread 7 / 200 ...\n",
      "processed thread 8 / 200 ...\n",
      "processed thread 9 / 200 ...cessed 19    / 100 ...\n",
      "processed thread 10 / 200 ...\n",
      "processed thread 11 / 200 ...\n",
      "processed thread 12 / 200 ...ssed 7    / 100 ...\n",
      "processed thread 13 / 200 ...essed 49    / 100 ...\n",
      "processed thread 14 / 200 ...essed 85    / 100 ...\n",
      "processed thread 15 / 200 ...\n",
      "processed thread 16 / 200 ...\n",
      "processed thread 17 / 200 ...\n",
      "processed thread 18 / 200 ...\n",
      "processed thread 19 / 200 ...\n",
      "processed thread 20 / 200 ...ssed 94    / 100 ...\n",
      "processed thread 21 / 200 ...\n",
      "processed thread 22 / 200 ...\n",
      "processed thread 23 / 200 ...\n",
      "processed thread 24 / 200 ...\n",
      "processed thread 25 / 200 ...\n",
      "processed thread 26 / 200 ...\n",
      "processed thread 27 / 200 ...\n",
      "processed thread 28 / 200 ...\n",
      "processed thread 29 / 200 ...\n",
      "processed thread 30 / 200 ...\n",
      "processed thread 31 / 200 ...\n",
      "processed thread 32 / 200 ...\n",
      "processed thread 33 / 200 ...essed 39    / 100 ...\n",
      "processed thread 34 / 200 ...\n",
      "processed thread 35 / 200 ...\n",
      "processed thread 36 / 200 ...\n",
      "processed thread 37 / 200 ...\n",
      "processed thread 38 / 200 ...\n",
      "processed thread 39 / 200 ...\n",
      "processed thread 40 / 200 ...\n",
      "processed thread 41 / 200 ...\n",
      "processed thread 42 / 200 ...essed 1    / 100 ...\n",
      "processed thread 43 / 200 ...\n",
      "processed thread 44 / 200 ...\n",
      "processed thread 45 / 200 ...\n",
      "processed thread 46 / 200 ...\n",
      "processed thread 47 / 200 ...\n",
      "processed thread 48 / 200 ...\n",
      "processed thread 49 / 200 ...\n",
      "processed thread 50 / 200 ...\n",
      "processed thread 51 / 200 ...\n",
      "processed thread 52 / 200 ...essed 93    / 100 ...\n",
      "processed thread 53 / 200 ...essed 50    / 100 ...\n",
      "processed thread 54 / 200 ...\n",
      "processed thread 55 / 200 ...\n",
      "processed thread 56 / 200 ...\n",
      "processed thread 57 / 200 ...ssed 18    / 100 ...\n",
      "processed thread 58 / 200 ...\n",
      "processed thread 59 / 200 ...essed 90    / 100 ...\n",
      "processed thread 60 / 200 ...processed 28    / 100 ...\n",
      "processed thread 61 / 200 ...essed 40    / 100 ...\n",
      "processed thread 62 / 200 ...\n",
      "processed thread 63 / 200 ...essed 2    / 100 ...\n",
      "processed thread 64 / 200 ...\n",
      "processed thread 65 / 200 ...\n",
      "processed thread 66 / 200 ...\n",
      "processed thread 67 / 200 ...\n",
      "processed thread 68 / 200 ...\n",
      "processed thread 69 / 200 ...essed 57    / 100 ...\n",
      "processed thread 70 / 200 ...\n",
      "processed thread 71 / 200 ...essed 1    / 100 ...\n",
      "processed thread 72 / 200 ...\n",
      "processed thread 73 / 200 ...processed 80    / 100 ...\n",
      "processed thread 74 / 200 ...processed 99    / 100 ...\n",
      "processed thread 75 / 200 ...\n",
      "processed thread 76 / 200 ...\n",
      "processed thread 77 / 200 ...\n",
      "processed thread 78 / 200 ...ssed 80    / 100 ...\n",
      "processed thread 79 / 200 ...\n",
      "processed thread 80 / 200 ...\n",
      "processed thread 81 / 200 ...ssed 6    / 100 ...\n",
      "processed thread 82 / 200 ...\n",
      "processed thread 83 / 200 ...ssed 90    / 100 ...\n",
      "processed thread 84 / 200 ...\n",
      "processed thread 85 / 200 ...\n",
      "processed thread 86 / 200 ...\n",
      "processed thread 87 / 200 ...ssed 94    / 100 ...\n",
      "processed thread 88 / 200 ...\n",
      "processed thread 89 / 200 ...ssed 31    / 100 ...\n",
      "processed thread 90 / 200 ...\n",
      "processed thread 91 / 200 ...essed 95    / 100 ...\n",
      "processed thread 92 / 200 ...\n",
      "processed thread 93 / 200 ...\n",
      "processed thread 94 / 200 ...\n",
      "processed thread 95 / 200 ...\n",
      "processed thread 96 / 200 ...\n",
      "processed thread 97 / 200 ...\n",
      "processed thread 98 / 200 ...\n",
      "processed thread 99 / 200 ...\n",
      "processed thread 100 / 200 ...\n",
      "processed thread 101 / 200 ...\n",
      "processed thread 102 / 200 ...\n",
      "processed thread 103 / 200 ...\n",
      "processed thread 104 / 200 ...\n",
      "processed thread 105 / 200 ...ssed 54    / 100 ...\n",
      "processed thread 106 / 200 ...\n",
      "processed thread 107 / 200 ...\n",
      "processed thread 108 / 200 ...\n",
      "processed thread 109 / 200 ...\n",
      "processed thread 110 / 200 ...\n",
      "processed thread 111 / 200 ...\n",
      "processed thread 112 / 200 ...\n",
      "processed thread 113 / 200 ...\n",
      "processed thread 114 / 200 ...\n",
      "processed thread 115 / 200 ...ssed 30    / 100 ...\n",
      "processed thread 116 / 200 ...\n",
      "processed thread 117 / 200 ...\n",
      "processed thread 118 / 200 ...ssed 5    / 100 ...\n",
      "processed thread 119 / 200 ...sed 98    / 100 ...\n",
      "processed thread 120 / 200 ...\n",
      "processed thread 121 / 200 ...ssed 1    / 100 ...\n",
      "processed thread 122 / 200 ...\n",
      "processed thread 123 / 200 ...\n",
      "processed thread 124 / 200 ...\n",
      "processed thread 125 / 200 ...ssed 66    / 100 ...\n",
      "processed thread 126 / 200 ...ssed 70    / 100 ...\n",
      "processed thread 127 / 200 ...ssed 15    / 100 ...\n",
      "processed thread 128 / 200 ...\n",
      "processed thread 129 / 200 ...\n",
      "processed thread 130 / 200 ...\n",
      "processed thread 131 / 200 ...ssed 84    / 100 ...\n",
      "processed thread 132 / 200 ...ssed 1    / 100 ...\n",
      "processed thread 133 / 200 ...\n",
      "processed thread 134 / 200 ...\n",
      "processed thread 135 / 200 ...\n",
      "processed thread 136 / 200 ...sed 100    / 100 ...\n",
      "processed thread 137 / 200 ...\n",
      "processed thread 138 / 200 ...\n",
      "processed thread 139 / 200 ...\n",
      "processed thread 140 / 200 ...\n",
      "processed thread 141 / 200 ...ssed 16    / 100 ...\n",
      "processed thread 142 / 200 ...\n",
      "processed thread 143 / 200 ...\n",
      "processed thread 144 / 200 ...\n",
      "processed thread 145 / 200 ...\n",
      "processed thread 146 / 200 ...\n",
      "processed thread 147 / 200 ...\n",
      "processed 55    / 100 ...processed thread 148 / 200 ...\n",
      "processed 1    / 100 ....0 ...processed 73    / 100 ...\n",
      "processed thread 150 / 200 ...\n",
      "processed thread 151 / 200 ...processed 33    / 100 ...\n",
      "processed thread 152 / 200 ...\n",
      "processed thread 153 / 200 ...\n",
      "processed thread 154 / 200 ...\n",
      "processed thread 155 / 200 ...sed 94    / 100 ...\n",
      "processed thread 156 / 200 ...processed 1    / 100 ...\n",
      "processed thread 157 / 200 ...\n",
      "processed thread 158 / 200 ...\n",
      "processed thread 159 / 200 ...ssed 5    / 100 ...\n",
      "processed thread 160 / 200 ...\n",
      "processed thread 161 / 200 ...sed 54    / 100 ...\n",
      "processed thread 162 / 200 ...\n",
      "processed thread 163 / 200 ...ssed 30    / 100 ...\n",
      "processed thread 164 / 200 ...\n",
      "processed thread 165 / 200 ...ssed 1    / 100 ...\n",
      "processed thread 166 / 200 ...\n",
      "processed thread 167 / 200 ...\n",
      "processed thread 168 / 200 ...\n",
      "processed thread 169 / 200 ...\n",
      "processed thread 170 / 200 ...\n",
      "processed thread 171 / 200 ...\n",
      "processed thread 172 / 200 ...sed 33    / 100 ...\n",
      "processed thread 173 / 200 ...ssed 58    / 100 ...\n",
      "processed thread 174 / 200 ...ssed 73    / 100 ...\n",
      "processed thread 175 / 200 ...\n",
      "processed thread 176 / 200 ...sed 74    / 100 ...\n",
      "processed thread 177 / 200 ...ssed 31    / 100 ...\n",
      "processed thread 178 / 200 ...\n",
      "processed 1    / 100 ...00 ...processed 22    / 100 ...\n",
      "processed thread 180 / 200 ...\n",
      "processed thread 181 / 200 ...\n",
      "processed thread 182 / 200 ...processed 97    / 100 ...\n",
      "processed thread 183 / 200 ...\n",
      "processed thread 184 / 200 ...\n",
      "processed thread 185 / 200 ...\n",
      "processed thread 186 / 200 ...sed 72    / 100 ...\n",
      "processed thread 187 / 200 ...ssed 10    / 100 ...\n",
      "processed thread 188 / 200 ...\n",
      "processed 73    / 100 ...processed thread 189 / 200 ...\n",
      "processed thread 190 / 200 ...\n",
      "processed thread 191 / 200 ...\n",
      "processed thread 192 / 200 ...processed 22    / 100 ...\n",
      "processed thread 193 / 200 ...processed 80    / 100 ...\n",
      "processed thread 194 / 200 ...\n",
      "processed thread 195 / 200 ...ssed 3    / 100 ...\n",
      "processed thread 196 / 200 ...\n",
      "processed thread 197 / 200 ...\n",
      "processed thread 198 / 200 ...\n",
      "processed thread 199 / 200 ...ssed 88    / 100 ...\n",
      "processed thread 200 / 200 ...\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "BATCHSIZE = 100\n",
    "MAX_WORKER = 4\n",
    "\n",
    "def batching_files(folder_path,batchsize):\n",
    "        all_files = os.listdir(folder_path)\n",
    "        total = len(all_files)\n",
    "        return [all_files[offset - batchsize : offset ] for offset in range(batchsize,total+batchsize,batchsize) if all_files[offset - batchsize : offset]]\n",
    "\n",
    "def read_corpus_batches(batch, folder_path):\n",
    "        batch_text = \"\"\n",
    "        i = 0\n",
    "        for fname in batch:\n",
    "            if fname.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                    batch_text += f.read().lower() + \"\\n\"\n",
    "                i+=1\n",
    "                print(f'processed {i}    / {len(batch)} ...'  , end=\"\\r\")\n",
    "        return batch_text\n",
    "\n",
    "def read_corpus( folder_path):\n",
    "        text = \"\"\n",
    "        print(\"Location of clean data\" , folder_path)\n",
    "        batches = batching_files(folder_path,BATCHSIZE)\n",
    "        threads = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(MAX_WORKER) as executor:\n",
    "            for batch in batches:    \n",
    "                threads.append(executor.submit(read_corpus_batches,batch,folder_path))\n",
    "            for idx,thread in enumerate( concurrent.futures.as_completed(threads)):\n",
    "                text += thread.result() \n",
    "                print(f'processed thread {idx+1} / {len(batches)} ...')\n",
    "        return text\n",
    "\n",
    "text = read_corpus('../data/clean/wiki_crawl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab41c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clean/wiki_crawl.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088ccaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clean/wiki_crawl.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compiled_pattern = re.compile(GPT4_SPLIT_PATTERN, flags=re.UNICODE | re.MULTILINE | re.DOTALL)\n",
    "text_chunk = re.findall(compiled_pattern,text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01400337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c21f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " '.',\n",
       " ' douglas',\n",
       " ' stone',\n",
       " ' is',\n",
       " ' the',\n",
       " ' carl',\n",
       " ' morse',\n",
       " ' professor',\n",
       " ' of']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aa3ee8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': [97], 'merged': 0, 'token_id': 0},\n",
       " {'token': [46], 'merged': 0, 'token_id': 1},\n",
       " {'token': [32, 100, 111, 117, 103, 108, 97, 115], 'merged': 0, 'token_id': 2},\n",
       " {'token': [32, 115, 116, 111, 110, 101], 'merged': 0, 'token_id': 3},\n",
       " {'token': [32, 105, 115], 'merged': 0, 'token_id': 4},\n",
       " {'token': [32, 116, 104, 101], 'merged': 0, 'token_id': 5},\n",
       " {'token': [32, 99, 97, 114, 108], 'merged': 0, 'token_id': 6},\n",
       " {'token': [32, 109, 111, 114, 115, 101], 'merged': 0, 'token_id': 7},\n",
       " {'token': [32, 112, 114, 111, 102, 101, 115, 115, 111, 114],\n",
       "  'merged': 0,\n",
       "  'token_id': 8},\n",
       " {'token': [32, 111, 102], 'merged': 0, 'token_id': 9}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd48e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import concurrent\n",
    "# seems faster\n",
    "def get_pairs(ids , pairs=None, most_common_pairs=None):\n",
    "    \"\"\"\n",
    "    iterate over all the pairs and calculate the frequency for all of them\n",
    "    \"\"\"\n",
    "    pairs = {} if pairs is None else pairs\n",
    "    most_common_pairs = None if most_common_pairs is None else most_common_pairs\n",
    "    for i in range(len(ids['token'])-1):\n",
    "        pair = (ids['token'][i], ids['token'][i+1])\n",
    "        if pair not in pairs:\n",
    "            pairs[pair] = {'count': 1, 'tokens': [ids['token_id']]}\n",
    "        else:\n",
    "            pairs[pair]['count'] += 1\n",
    "            pairs[pair]['tokens'].append(ids['token_id'])\n",
    "        if most_common_pairs is None or pairs[pair]['count'] > pairs[most_common_pairs]['count']:\n",
    "            most_common_pairs = pair\n",
    "    return pairs,most_common_pairs\n",
    "\n",
    "def update_pairs(ids , pairs, new_pairs=None):\n",
    "    \"\"\"\n",
    "    iterate over all the pairs and calculate the frequency for all of them\n",
    "    \"\"\"\n",
    "    new_pairs = set() if new_pairs is None else new_pairs\n",
    "    for i in range(len(ids['token'])-1):\n",
    "        pair = (ids['token'][i], ids['token'][i+1])\n",
    "        if pair not in pairs:\n",
    "            new_pairs.add(pair)\n",
    "            pairs[pair] = {'count': 1, 'tokens': [ids['token_id']]}\n",
    "        elif pair in new_pairs:\n",
    "            pairs[pair]['count'] += 1\n",
    "            pairs[pair]['tokens'].append(ids['token_id'])\n",
    "    pair = max(pairs, key=pairs.get('count'))\n",
    "    return pairs , pair\n",
    "\n",
    "def get_pairs_parallel(ids,pairs):\n",
    "        \"\"\"\n",
    "        iterate over all the pairs and calculate the frequency for all of them\n",
    "        \"\"\"\n",
    "        pairs += Counter(zip(ids[:-1], ids[1:]))\n",
    "        return pairs\n",
    "\n",
    "def merge_tokens(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids['token']):\n",
    "        if ids['token'][i] == pair[0] and i < len(ids['token']) - 1 and ids['token'][i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            new_ids.append(ids['token'][i])\n",
    "            i +=1\n",
    "    ids['token'] = new_ids\n",
    "    return ids\n",
    "\n",
    "def batch_merge_tokens(batch, pair, idx):\n",
    "    \"\"\"\n",
    "    Merge tokens in a batch of ids based on the given pair and index.\n",
    "    \"\"\"\n",
    "    merged_batch = []\n",
    "    for ids in batch:\n",
    "        merged_ids = merge_tokens(ids, pair, idx)\n",
    "        merged_batch.append(merged_ids)\n",
    "    return merged_batch\n",
    "\n",
    "def merge_tokens_parallel(ids, pair, idx):\n",
    "    batches = [ids[i:i+1000] for i in range(0, len(ids), 1000)]\n",
    "    with concurrent.futures.ProcessPoolExecutor(5) as executor:\n",
    "        futures = [executor.submit(batch_merge_tokens, batch, pair, idx) for batch in batches]\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.extend(future.result())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997ae0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57f5bf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'merged': 0, 'token': [4, 5, 65, 7, 89], 'token_id': 1}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_ = [{'toeken': [4,5,65,7,89] , 'merged': -1, 'token_id': 1}]\n",
    "for i,chunk_ids in enumerate(ids_):\n",
    "    ids_[i]['merged'] = 0\n",
    "ids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3375c4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pairs = set()\n",
    "new_pairs.add('a')\n",
    "new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef37992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Most common pair: (32, 116) with count 9120022\n",
      "Vocabulary size: 257 , Merges: 1 , Pairs: 11448\n",
      "Iteration 2 - Most common pair: (257, 234) with count 1\n",
      "Vocabulary size: 258 , Merges: 2 , Pairs: 11487\n",
      "Iteration 3 - Most common pair: (258, 156) with count 1\n",
      "Vocabulary size: 259 , Merges: 3 , Pairs: 11487\n",
      "Iteration 4 - Most common pair: (259, 163) with count 1\n",
      "Vocabulary size: 260 , Merges: 4 , Pairs: 11487\n",
      "Iteration 5 - Most common pair: (257, 227) with count 1\n",
      "Vocabulary size: 261 , Merges: 5 , Pairs: 11486\n",
      "Iteration 6 - Most common pair: (261, 130) with count 1\n",
      "Vocabulary size: 262 , Merges: 6 , Pairs: 11486\n",
      "Iteration 7 - Most common pair: (262, 183) with count 1\n",
      "Vocabulary size: 263 , Merges: 7 , Pairs: 11486\n",
      "Iteration 8 - Most common pair: (263, 227) with count 1\n",
      "Vocabulary size: 264 , Merges: 8 , Pairs: 11486\n",
      "Iteration 9 - Most common pair: (264, 131) with count 1\n",
      "Vocabulary size: 265 , Merges: 9 , Pairs: 11486\n"
     ]
    }
   ],
   "source": [
    "ids = [{ 'token' : list(ch.encode(\"utf-8\")) , 'merged' : -1  , 'token_id': i} for i,ch in enumerate(text_chunk)]\n",
    "pairs,pair = None, None\n",
    "vocab_size = 266\n",
    "num_merge = vocab_size - 256\n",
    "merges = {}\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for idm in range(1,num_merge):\n",
    "    new_pairs = None\n",
    "    for i in range(len(ids)):\n",
    "        if ids[i]['merged'] == -1:\n",
    "            pairs , pair = get_pairs(ids[i],pairs, pair)\n",
    "        elif ids[i]['merged'] == 1:\n",
    "            pairs, pair = update_pairs(ids[i], pairs,new_pairs)\n",
    "        ids[i]['merged'] = 0\n",
    "    idx = 256 + idm\n",
    "    print(f\"Iteration {idm} - Most common pair: {pair} with count {pairs[pair]['count']}\")\n",
    "    for i in pairs[pair]['tokens']:\n",
    "        ids[i] = merge_tokens(ids[i], pair, idx)\n",
    "        ids[i]['merged'] = 1\n",
    "    merges[pair] = idx\n",
    "    pairs.pop(pair, None)\n",
    "    vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "    print(f\"Vocabulary size: {len(vocab)} , Merges: {len(merges)} , Pairs: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0da0bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(264, 131)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pairs[(\u001b[38;5;241m264\u001b[39m, \u001b[38;5;241m131\u001b[39m)]\n",
      "\u001b[1;31mKeyError\u001b[0m: (264, 131)"
     ]
    }
   ],
   "source": [
    "pairs[(264, 131)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd97ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(pairs, key\u001b[38;5;241m=\u001b[39mpairs\u001b[38;5;241m.\u001b[39mget)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "pair = max(pairs, key=pairs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131e95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = (32,116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18edbbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ids_ \u001b[38;5;241m=\u001b[39m merge_tokens_parallel(ids,pair,\u001b[38;5;241m257\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 49\u001b[0m, in \u001b[0;36mmerge_tokens_parallel\u001b[1;34m(ids, pair, idx)\u001b[0m\n\u001b[0;32m     47\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[1;32m---> 49\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\rdp\\anaconda3\\envs\\artemis\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\rdp\\anaconda3\\envs\\artemis\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "ids_ = merge_tokens_parallel(ids,pair,257)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4025c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_2 = [merge_tokens(chunk_ids, pair, 257) for chunk_ids in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd688e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  Merge Done 1 / 15744 \n",
      "                  Vocab size: 257   \n",
      "                  New Ids: 80671071    \n",
      "                  Work time for Get Pairs: 53.64 seconds  \n",
      "                  Work time for Merge: 73.86 seconds\n",
      "                  ______________________________________________________\n",
      "                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_ids \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m---> 10\u001b[0m     pairs \u001b[38;5;241m=\u001b[39m get_pairs(chunk_ids, pairs)\n\u001b[0;32m     11\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(pairs, key\u001b[38;5;241m=\u001b[39mpairs\u001b[38;5;241m.\u001b[39mget)\n\u001b[0;32m     12\u001b[0m elapsed_get_pairs \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mget_pairs\u001b[1;34m(ids, pairs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03miterate over all the pairs and calculate the frequency for all of them\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pairs \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pairs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pairs\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     10\u001b[0m         pair \u001b[38;5;241m=\u001b[39m (ids[i], ids[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     11\u001b[0m         pairs[pair] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "vocab_size = 16000\n",
    "num_merge = vocab_size - 256\n",
    "merges = {}\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for i in range(num_merge):\n",
    "    start = time.time()\n",
    "    pairs = None\n",
    "    for chunk_ids in ids:\n",
    "        pairs = get_pairs(chunk_ids, pairs)\n",
    "    pair = max(pairs, key=pairs.get)\n",
    "    elapsed_get_pairs = time.time() - start\n",
    "    idx = 256 + i\n",
    "    start = time.time()\n",
    "    ids = [merge_tokens(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "    merges[pair] = idx\n",
    "    vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "    elapsed_merge = time.time() - start\n",
    "    print(f\"\"\"\n",
    "                  Merge Done {i+1} / {num_merge} \n",
    "                  Vocab size: {len(vocab)}   \n",
    "                  New Ids: {len(ids)}    \n",
    "                  Work time for Get Pairs: {elapsed_get_pairs:.2f} seconds  \n",
    "                  Work time for Merge: {elapsed_merge:.2f} seconds\n",
    "                  ______________________________________________________\n",
    "                  \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a44aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 116)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22f919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6440cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "070b517f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[97],\n",
       " [46],\n",
       " [32, 100, 111, 117, 103, 108, 97, 115],\n",
       " [32, 115, 116, 111, 110, 101],\n",
       " [32, 105, 115],\n",
       " [257, 104, 101],\n",
       " [32, 99, 97, 114, 108],\n",
       " [32, 109, 111, 114, 115, 101],\n",
       " [32, 112, 114, 111, 102, 101, 115, 115, 111, 114],\n",
       " [32, 111, 102]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5f0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artemis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
